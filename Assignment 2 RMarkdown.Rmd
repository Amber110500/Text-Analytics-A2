---
title: "Assignment 2 Text Analytics"
author: "Robbert Batenburg"
date: "2024-04-17"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


skip_slow <- TRUE

# Set the maximum number of rows to be printed
options(max.print = 10000)

library(dplyr)
library(tidyr)
library(text2vec)
library(tidytext)
library(ggplot2)
library(SnowballC)

```

```{r data}
#load data
setwd("C:/Users/robbe/OneDrive/uni/Master DSMA/Text Analysis/Assignment 2 RMarkdown")

# Load data
#disney_unique <- load(file = "disney_stemmed_new.RData")
```


```{r}

# Create small dataset to work with
#reviews_df <- disney_unique[disney_unique$Branch == "Disneyland_HongKong",]
#reviews_df <- reviews_df[1:10000,]
#reviews_df$Description <- reviews_df$Review_Text

```

## Word stemming

```{r , echo=FALSE, message=FALSE}

# Apply word stemming
#for (j in 1:nrow(reviews_df)) {
  # Also remove stop words
#  stemmed_description<-  anti_join((reviews_df[j,] %>% unnest_tokens(word,Description, drop=FALSE,to_lower=TRUE) ),stop_words)

#    stemmed_description<-  ((reviews_df[j,] %>% unnest_tokens(word,Description, drop=FALSE,to_lower=TRUE) ))
#    stemmed_description<-(wordStem(stemmed_description[,"word"], language = "porter"))

    # Add to dataframe
#    reviews_df[j, "stemmed_reviewtext"] <- paste(stemmed_description, collapse = " ")

#}
#print("done")

#stemmed_reviews_df_with_stop <- reviews_df

#save(stemmed_reviews_df_with_stop , file = "stemmed_reviews_df_with_stop.Rdata")


  load(file = "stemmed_reviews_df_with_stop.Rdata")
  stemmed_reviews_df_with_stop -> reviews_df

```


```{r}
# create iterator over list of text items
it = itoken(reviews_df$stemmed_reviewtext)

#create the vocabulary and remove infrequent words
vocabulary = create_vocabulary(it)
vocabulary = prune_vocabulary(vocabulary, term_count_min = 100 )

#create vector version of the vocabulary: speeds up allocation/search process
v_vect = vocab_vectorizer(vocabulary)
# create term co-occurrence matrix (window of 8)
tcm = create_tcm(it, v_vect, skip_grams_window = 8L, skip_grams_window_context = "symmetric", weights = rep(1,8) ) 
```


```{r}
#Create wordlist

wordlist = c("food", "lunch" , "restaur", "attract", "bad", "disappoint", "ride", "fantast", "husband", "wife", "daughter", "son")

```


# Create the GloVe model

```{r}

glove_model = GlobalVectors$new( x_max = 1000 , rank = 50) 
# use maximum 1000 word co-occurrences in weight function
# rank is the desired dimension for latent vectors
```


# Fit model and obtain results.

# note that the glove_model construct is now also changed!

```{r}
# Fit the model using 200 stochastic gradient descent (SGD) iterations
word_vectors = glove_model$fit_transform(tcm, n_iter = 200)

word_vectors_backup <- word_vectors # save as backup


```


```{r}

# Show first 50 dimensions
t(word_vectors[wordlist[1:4],1:50])

```


# Determine pairwise word similarities

```{r}

similarity_matrix = sim2(word_vectors)
print("a sample of the similarity matrix across words ")
similarity_matrix[wordlist,wordlist]
nwords=nrow(similarity_matrix)

```


## Finding similar words

```{r}
# vector to select first 10 and last 10 words
top_bottom=c(1:10,(nwords-9):nwords)
```

```{r}
# Select words with top 10 highest and lowest similarity score to curri
cc=sort(similarity_matrix[,"fantast"], decreasing=TRUE)
print(data.frame(cc[top_bottom]))

```














### REPORT

To analyse the reviews about Disneyland Hong Kong, we will first use a GloVe algorithm to embed the words used in the reviews. The Glove model results in many vectors across different dimensions, or aiming in different "directions", which mathematically describe the meaning of the words. When words are similar to each other in a dimension, their vectors aim towards the same direction. This means that their values in this dimensions are both either positive or both negative. Words that have a similar meaning, will have vectors close to each other in many dimensions.

Table 2 shows a sample of the results of our GloVe model. The first and second row shows dimensions where bad and disappoint have the same direction and the opposite direction to fantast. These dimensions can help capture the difference between positive and negative sentiment. Food, lunch and restaurant have negative directions in the first dimension. This direction therefore helps to capture the meaning of food. These food related terms also have the same direction as fantast and the opposite direction to bad and disappoint, which indicates that there is a positive sentiment about the food in Disneyland Honk Kong. 

The first direction has the highest score for food. In other words, the word food has the biggest vector in this dimension. Food has the higher semantic representation along this particular dimension. Other words with negative vectors in this dimension are more likely to have a related meaning towards food, because their semantic attribute in this dimension is similar.


```{r}
#t(word_vectors[,c(48)])


#t(word_vectors[wordlist[c(1:12)],c(1, 3, 6)])

t(word_vectors[wordlist[c(1, 2, 3, 5, 6, 8, 9, 10, 11, 12)],c(3, 6, 48)])
```

```{r}

# Show first 50 dimensions
t(word_vectors[,46])
```


#subvraag 2

```{r}

# Show first 50 dimensions
t(word_vectors[wordlist[c(1:13)],25])
```

## subvraag 3 Doing word arithmetic (adding and subtracting vectors)

```{r}
# Relationships food similar to restaur, but including the difference between food and ride
comparator = word_vectors["restaur",] - word_vectors["food",] + word_vectors["ride",] 
similarities = sim2(word_vectors,t(comparator))
ranking = similarities %>% order(decreasing=TRUE)
print(as.data.frame(similarities[ranking[top_bottom]], row.names = vocabulary$term[ranking[top_bottom]]))
```